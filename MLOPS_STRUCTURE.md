# Production-Ready MLOps Project Structure

## RL-Based CDSS: From Research to Production

**Author:** Herald Michain Samuel Theo Ginting  
**Role Context:** Senior MLOps Engineer & AI Researcher  
**Date:** January 2, 2026

---

## ðŸ“ COMPLETE FOLDER STRUCTURE

```
adaptive-cdss-under-uncertainty/
â”‚
â”œâ”€â”€ .github/                          # GitHub-specific configurations
â”‚   â”œâ”€â”€ workflows/                    # CI/CD pipelines
â”‚   â”‚   â”œâ”€â”€ ci.yml                   # Continuous Integration (tests, lint)
â”‚   â”‚   â”œâ”€â”€ cd.yml                   # Continuous Deployment (Docker build, push)
â”‚   â”‚   â”œâ”€â”€ mlflow-tracking.yml      # Auto-log experiments to MLflow
â”‚   â”‚   â””â”€â”€ data-validation.yml      # DVC pipeline validation
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/              # Issue templates
â”‚   â””â”€â”€ PULL_REQUEST_TEMPLATE.md     # PR template
â”‚
â”œâ”€â”€ configs/                          # Configuration management (CRITICAL!)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.yaml                    # Base configuration (hyperparams, paths)
â”‚   â”œâ”€â”€ experiment/                  # Experiment-specific configs
â”‚   â”‚   â”œâ”€â”€ q_learning_baseline.yaml
â”‚   â”‚   â”œâ”€â”€ sarsa_conservative.yaml
â”‚   â”‚   â””â”€â”€ fuzzy_reward.yaml
â”‚   â”œâ”€â”€ data/                        # Data configurations
â”‚   â”‚   â”œâ”€â”€ missing_20.yaml          # 20% missing data scenario
â”‚   â”‚   â”œâ”€â”€ missing_40.yaml          # 40% baseline
â”‚   â”‚   â””â”€â”€ missing_60.yaml          # 60% stress test
â”‚   â””â”€â”€ deployment/                  # Deployment configs
â”‚       â”œâ”€â”€ dev.yaml
â”‚       â”œâ”€â”€ staging.yaml
â”‚       â””â”€â”€ prod.yaml
â”‚
â”œâ”€â”€ data/                            # Data directory (DVC-tracked)
â”‚   â”œâ”€â”€ raw/                         # Original, immutable data
â”‚   â”‚   â”œâ”€â”€ drugbank/
â”‚   â”‚   â”œâ”€â”€ ddinter/
â”‚   â”‚   â””â”€â”€ fda_labels/
â”‚   â”œâ”€â”€ processed/                   # Cleaned, transformed data
â”‚   â”‚   â”œâ”€â”€ drugs.json
â”‚   â”‚   â”œâ”€â”€ interactions.json
â”‚   â”‚   â””â”€â”€ contraindications.json
â”‚   â”œâ”€â”€ synthetic/                   # Generated synthetic patients
â”‚   â”‚   â”œâ”€â”€ train_patients.pkl
â”‚   â”‚   â””â”€â”€ test_patients.pkl
â”‚   â””â”€â”€ external/                    # Third-party data sources
â”‚       â””â”€â”€ references/
â”‚
â”œâ”€â”€ models/                          # Trained models (DVC + MLflow registry)
â”‚   â”œâ”€â”€ checkpoints/                 # Training checkpoints
â”‚   â”‚   â”œâ”€â”€ q_learning_ep500.pkl
â”‚   â”‚   â””â”€â”€ sarsa_ep500.pkl
â”‚   â”œâ”€â”€ production/                  # Deployed models (MLflow registry)
â”‚   â”‚   â”œâ”€â”€ q_learning_v1.0.pkl
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ experimental/                # Experimental models
â”‚       â””â”€â”€ fuzzy_reward_beta.pkl
â”‚
â”œâ”€â”€ notebooks/                       # Jupyter notebooks (exploratory ONLY)
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_baseline_analysis.ipynb
â”‚   â”œâ”€â”€ 03_training_debug.ipynb
â”‚   â””â”€â”€ 04_results_visualization.ipynb
â”‚
â”œâ”€â”€ src/                             # Source code (main application)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ knowledge/                   # Knowledge Base module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py        # KnowledgeBase class
â”‚   â”‚   â”œâ”€â”€ loader.py                # Data loading utilities
â”‚   â”‚   â””â”€â”€ validator.py             # Consistency validation
â”‚   â”‚
â”‚   â”œâ”€â”€ environment/                 # RL Environment module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cdss_env.py              # Main CDSSEnvironment class
â”‚   â”‚   â”œâ”€â”€ patient_generator.py    # Synthetic patient generation
â”‚   â”‚   â”œâ”€â”€ observation_model.py    # Partial observability simulation
â”‚   â”‚   â””â”€â”€ reward.py                # Reward function implementations
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                      # RL Agents module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py            # Abstract base class
â”‚   â”‚   â”œâ”€â”€ q_learning.py            # Q-Learning agent
â”‚   â”‚   â”œâ”€â”€ sarsa.py                 # SARSA agent
â”‚   â”‚   â”œâ”€â”€ state_encoding.py       # Belief state approximation
â”‚   â”‚   â””â”€â”€ baselines/               # Baseline policies
â”‚   â”‚       â”œâ”€â”€ random_policy.py
â”‚   â”‚       â”œâ”€â”€ rule_based.py
â”‚   â”‚       â””â”€â”€ oracle.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                    # Training orchestration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py               # Main training loop
â”‚   â”‚   â”œâ”€â”€ callbacks.py             # Training callbacks (early stopping, etc.)
â”‚   â”‚   â””â”€â”€ logger.py                # MLflow integration wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                  # Evaluation & metrics
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py               # SafetyMetrics class
â”‚   â”‚   â”œâ”€â”€ robustness.py            # Robustness testing
â”‚   â”‚   â”œâ”€â”€ comparator.py            # Policy comparison
â”‚   â”‚   â””â”€â”€ statistical.py           # Statistical validation (t-tests, CIs)
â”‚   â”‚
â”‚   â”œâ”€â”€ explainability/              # XAI & interpretability
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ explainer.py             # Decision explanation
â”‚   â”‚   â”œâ”€â”€ q_analyzer.py            # Q-value analysis
â”‚   â”‚   â””â”€â”€ visualizer.py            # Explanation visualizations
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config_loader.py         # YAML config loading
â”‚   â”‚   â”œâ”€â”€ reproducibility.py       # Seed setting, determinism
â”‚   â”‚   â””â”€â”€ io.py                    # File I/O helpers
â”‚   â”‚
â”‚   â””â”€â”€ api/                         # REST API for model serving (Optional)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py                   # FastAPI application
â”‚       â”œâ”€â”€ models.py                # Pydantic schemas
â”‚       â””â”€â”€ endpoints.py             # API endpoints
â”‚
â”œâ”€â”€ tests/                           # Testing suite (CRITICAL!)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/                        # Unit tests
â”‚   â”‚   â”œâ”€â”€ test_knowledge_base.py
â”‚   â”‚   â”œâ”€â”€ test_environment.py
â”‚   â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”‚   â””â”€â”€ test_reward.py
â”‚   â”œâ”€â”€ integration/                 # Integration tests
â”‚   â”‚   â”œâ”€â”€ test_training_pipeline.py
â”‚   â”‚   â””â”€â”€ test_evaluation_pipeline.py
â”‚   â”œâ”€â”€ performance/                 # Performance/benchmark tests
â”‚   â”‚   â””â”€â”€ test_inference_speed.py
â”‚   â””â”€â”€ fixtures/                    # Test fixtures & mocks
â”‚       â”œâ”€â”€ sample_patients.json
â”‚       â””â”€â”€ mock_knowledge_base.json
â”‚
â”œâ”€â”€ scripts/                         # Standalone scripts
â”‚   â”œâ”€â”€ data_acquisition/           # Data collection scripts
â”‚   â”‚   â”œâ”€â”€ fetch_drugbank.py
â”‚   â”‚   â”œâ”€â”€ fetch_ddinter.py
â”‚   â”‚   â””â”€â”€ generate_synthetic_patients.py
â”‚   â”œâ”€â”€ preprocessing/              # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ clean_interactions.py
â”‚   â”‚   â””â”€â”€ validate_knowledge_base.py
â”‚   â”œâ”€â”€ experiments/                # Experiment runners
â”‚   â”‚   â”œâ”€â”€ run_baseline_comparison.py
â”‚   â”‚   â”œâ”€â”€ run_robustness_test.py
â”‚   â”‚   â””â”€â”€ run_ablation_study.py
â”‚   â””â”€â”€ deployment/                 # Deployment scripts
â”‚       â”œâ”€â”€ build_docker.sh
â”‚       â””â”€â”€ deploy_to_mlflow.py
â”‚
â”œâ”€â”€ mlruns/                         # MLflow tracking directory (gitignored)
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ .dvc/                           # DVC configuration (auto-generated)
â”‚   â””â”€â”€ config
â”‚
â”œâ”€â”€ docker/                         # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile                  # Main application Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.mlflow           # MLflow server Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.api              # API server Dockerfile
â”‚   â””â”€â”€ docker-compose.yml          # Multi-container orchestration
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ api/                        # API documentation
â”‚   â”œâ”€â”€ architecture/               # Architecture diagrams
â”‚   â”‚   â”œâ”€â”€ ARCHITECTURE.md         # (Your existing file)
â”‚   â”‚   â””â”€â”€ diagrams/
â”‚   â”œâ”€â”€ research/                   # Research papers & reports
â”‚   â”‚   â”œâ”€â”€ research_paper.pdf
â”‚   â”‚   â”œâ”€â”€ theoretical_foundation.md
â”‚   â”‚   â””â”€â”€ performance_metrics.md
â”‚   â””â”€â”€ guides/                     # User guides
â”‚       â”œâ”€â”€ quickstart.md
â”‚       â”œâ”€â”€ training_guide.md
â”‚       â””â”€â”€ deployment_guide.md
â”‚
â”œâ”€â”€ logs/                           # Application logs (gitignored)
â”‚   â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ api/
â”‚
â”œâ”€â”€ results/                        # Experiment results (DVC-tracked)
â”‚   â”œâ”€â”€ figures/                    # Generated plots
â”‚   â”‚   â”œâ”€â”€ learning_curves/
â”‚   â”‚   â”œâ”€â”€ robustness_plots/
â”‚   â”‚   â””â”€â”€ comparison_charts/
â”‚   â”œâ”€â”€ metrics/                    # Metrics JSON/CSV
â”‚   â”‚   â”œâ”€â”€ baseline_comparison.json
â”‚   â”‚   â””â”€â”€ robustness_results.csv
â”‚   â””â”€â”€ reports/                    # Auto-generated reports
â”‚       â””â”€â”€ experiment_summary.html
â”‚
â”œâ”€â”€ deployments/                    # Deployment artifacts
â”‚   â”œâ”€â”€ kubernetes/                 # K8s manifests (if applicable)
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â””â”€â”€ service.yaml
â”‚   â””â”€â”€ mlflow_models/              # MLflow model artifacts
â”‚       â””â”€â”€ production_v1/
â”‚
â”œâ”€â”€ .dvcignore                      # DVC ignore patterns
â”œâ”€â”€ .gitignore                      # Git ignore patterns
â”œâ”€â”€ .dockerignore                   # Docker ignore patterns
â”œâ”€â”€ .env.example                    # Environment variables template
â”œâ”€â”€ .pre-commit-config.yaml         # Pre-commit hooks (linting, formatting)
â”‚
â”œâ”€â”€ pyproject.toml                  # Project metadata & dependencies (Poetry/setuptools)
â”œâ”€â”€ requirements.txt                # Python dependencies (pip)
â”œâ”€â”€ requirements-dev.txt            # Development dependencies
â”œâ”€â”€ setup.py                        # Package installation script
â”‚
â”œâ”€â”€ dvc.yaml                        # DVC pipeline definition
â”œâ”€â”€ dvc.lock                        # DVC pipeline lock file
â”œâ”€â”€ params.yaml                     # DVC parameters (hyperparameters)
â”‚
â”œâ”€â”€ Makefile                        # Task automation (CRITICAL!)
â”œâ”€â”€ README.md                       # Project overview
â”œâ”€â”€ LICENSE                         # MIT License
â””â”€â”€ CONTRIBUTING.md                 # Contribution guidelines
```

---

## ðŸ”¥ KEY COMPONENTS EXPLAINED

### 1. **`configs/` - Configuration Management** â­ CRITICAL

**Why Essential:**

- Separates hyperparameters from code
- Enables reproducible experiments
- Easy experiment tracking in MLflow

**Structure:**

```yaml
# configs/base.yaml
project:
  name: "rl-cdss-prescription-safety"
  author: "Herald Ginting"

training:
  n_episodes: 500
  alpha: 0.1
  gamma: 0.95
  epsilon: 0.2

environment:
  missing_rate: 0.4
  drugs:
    - warfarin
    - aspirin
    # ...

mlflow:
  tracking_uri: "http://localhost:5000"
  experiment_name: "q-learning-baseline"
```

**Usage:**

```python
from src.utils.config_loader import load_config

config = load_config("configs/experiment/q_learning_baseline.yaml")
agent = QLearningAgent(
    alpha=config.training.alpha,
    gamma=config.training.gamma
)
```

---

### 2. **MLflow Integration** ðŸŽ¯

**Three Critical Files:**

#### A. `src/training/logger.py` - MLflow Wrapper

```python
import mlflow
from src.utils.config_loader import load_config

class MLflowLogger:
    def __init__(self, experiment_name, run_name=None):
        config = load_config("configs/base.yaml")
        mlflow.set_tracking_uri(config.mlflow.tracking_uri)
        mlflow.set_experiment(experiment_name)
        self.run_name = run_name
    
    def log_params(self, params):
        """Log hyperparameters"""
        mlflow.log_params(params)
    
    def log_metrics(self, metrics, step=None):
        """Log metrics (detection_rate, etc.)"""
        mlflow.log_metrics(metrics, step=step)
    
    def log_artifact(self, artifact_path):
        """Log file artifacts (models, plots)"""
        mlflow.log_artifact(artifact_path)
    
    def log_model(self, model, model_name):
        """Log model to MLflow Model Registry"""
        mlflow.sklearn.log_model(model, model_name)
```

#### B. `scripts/experiments/run_baseline_comparison.py`

```python
import mlflow
from src.training.logger import MLflowLogger
from src.agents.q_learning import QLearningAgent
from src.training.trainer import Trainer

def main():
    logger = MLflowLogger(
        experiment_name="baseline-comparison",
        run_name="q-learning-missing-40"
    )
    
    with mlflow.start_run(run_name=logger.run_name):
        # Log config
        logger.log_params({
            "algorithm": "q-learning",
            "alpha": 0.1,
            "gamma": 0.95,
            "missing_rate": 0.4
        })
        
        # Train
        agent = QLearningAgent(alpha=0.1, gamma=0.95)
        trainer = Trainer(env, agent)
        history = trainer.train(n_episodes=500)
        
        # Log metrics
        logger.log_metrics({
            "final_reward": history["rewards"][-1],
            "convergence_episode": trainer.convergence_episode
        })
        
        # Log model
        logger.log_model(agent, "q_learning_agent")
        
        # Log artifacts
        logger.log_artifact("results/figures/learning_curve.png")

if __name__ == "__main__":
    main()
```

#### C. **MLflow UI Access**

```bash
# Start MLflow server
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000

# Access: http://localhost:5000
```

**MLflow Workflow:**

1. **Tracking:** Every experiment auto-logs to MLflow (metrics, params, artifacts)
2. **Comparison:** MLflow UI shows all runs side-by-side
3. **Registry:** Best model promoted to "Production" stage
4. **Deployment:** Load model from registry for serving

---

### 3. **DVC (Data Version Control)** ðŸ“Š

**Purpose:** Version control for data & models (Git for large files)

**Key Files:**

#### `dvc.yaml` - Pipeline Definition

```yaml
stages:
  data_acquisition:
    cmd: python scripts/data_acquisition/fetch_drugbank.py
    deps:
      - scripts/data_acquisition/fetch_drugbank.py
    outs:
      - data/raw/drugbank/

  preprocessing:
    cmd: python scripts/preprocessing/clean_interactions.py
    deps:
      - data/raw/drugbank/
      - scripts/preprocessing/clean_interactions.py
    outs:
      - data/processed/interactions.json

  training:
    cmd: python scripts/experiments/run_baseline_comparison.py
    deps:
      - data/processed/interactions.json
      - src/
    params:
      - configs/base.yaml:
          - training
    outs:
      - models/checkpoints/q_learning_ep500.pkl
    metrics:
      - results/metrics/baseline_comparison.json:
          cache: false

  evaluation:
    cmd: python scripts/experiments/run_robustness_test.py
    deps:
      - models/checkpoints/q_learning_ep500.pkl
    outs:
      - results/figures/robustness_plots/
```

**DVC Commands:**

```bash
# Initialize DVC
dvc init

# Track large files
dvc add data/raw/drugbank/
dvc add models/checkpoints/q_learning_ep500.pkl

# Run pipeline
dvc repro

# Push data to remote (S3, Google Drive, etc.)
dvc remote add -d myremote s3://my-bucket/dvc-storage
dvc push

# Reproduce experiment on different machine
dvc pull
dvc repro
```

---

### 4. **Docker Configuration** ðŸ³

**Three Dockerfiles for Different Purposes:**

#### `docker/Dockerfile` - Main Application

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY src/ ./src/
COPY configs/ ./configs/
COPY models/ ./models/

# Run training
CMD ["python", "-m", "scripts.experiments.run_baseline_comparison"]
```

#### `docker/Dockerfile.mlflow` - MLflow Server

```dockerfile
FROM python:3.9-slim

RUN pip install mlflow boto3 psycopg2-binary

EXPOSE 5000

CMD ["mlflow", "server", \
     "--backend-store-uri", "postgresql://user:pass@db:5432/mlflow", \
     "--default-artifact-root", "s3://my-bucket/mlflow-artifacts", \
     "--host", "0.0.0.0"]
```

#### `docker/docker-compose.yml` - Multi-Container

```yaml
version: '3.8'

services:
  mlflow:
    build:
      context: ..
      dockerfile: docker/Dockerfile.mlflow
    ports:
      - "5000:5000"
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

  training:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    volumes:
      - ../data:/app/data
      - ../models:/app/models
      - ../results:/app/results
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - mlflow

  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.api
    ports:
      - "8000:8000"
    depends_on:
      - mlflow
```

**Usage:**

```bash
# Build & run all services
docker-compose -f docker/docker-compose.yml up --build

# MLflow UI: http://localhost:5000
# API: http://localhost:8000/docs
```

---

### 5. **`tests/` - Testing Strategy** âœ…

**Coverage Target: >80%**

**Structure:**

#### Unit Tests

```python
# tests/unit/test_reward.py
import pytest
from src.environment.reward import RewardFunction
from src.knowledge.knowledge_base import KnowledgeBase

@pytest.fixture
def reward_fn():
    kb = KnowledgeBase("tests/fixtures/mock_knowledge_base.json")
    return RewardFunction(kb)

def test_safe_approval_reward(reward_fn):
    patient = create_safe_patient()  # No DDI
    reward = reward_fn.compute(patient, action=0)  # APPROVE
    assert reward == 2, "Safe approval should return +2"

def test_severe_interaction_missed(reward_fn):
    patient = create_high_risk_patient()  # Warfarin + Aspirin
    reward = reward_fn.compute(patient, action=0)  # APPROVE
    assert reward == -10, "Missed severe interaction should return -10"
```

#### Integration Tests

```python
# tests/integration/test_training_pipeline.py
def test_full_training_pipeline():
    """Test complete training â†’ evaluation flow"""
    env = CDSSEnvironment(knowledge_path="tests/fixtures/")
    agent = QLearningAgent(alpha=0.1, gamma=0.95, epsilon=0.1)
    trainer = Trainer(env, agent)
    
    # Train for 10 episodes (fast test)
    history = trainer.train(n_episodes=10)
    
    assert len(history["rewards"]) == 10
    assert "td_errors" in history
    assert agent.q_table  # Q-table populated
```

**Run Tests:**

```bash
# All tests
pytest tests/ -v --cov=src --cov-report=html

# Specific test suite
pytest tests/unit/ -v

# Coverage report
open htmlcov/index.html
```

---

### 6. **CI/CD Pipeline** ðŸ”„

**`.github/workflows/ci.yml`:**

```yaml
name: CI Pipeline

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run tests
        run: pytest tests/ --cov=src --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
      
      - name: Lint code
        run: |
          flake8 src/ --max-line-length=100
          mypy src/
```

---

### 7. **`Makefile` - Task Automation** âš¡

```makefile
.PHONY: install test train evaluate docker-build clean

install:
 pip install -r requirements.txt
 pip install -r requirements-dev.txt
 dvc pull

test:
 pytest tests/ -v --cov=src --cov-report=html

lint:
 flake8 src/ --max-line-length=100
 mypy src/
 black --check src/

format:
 black src/ tests/
 isort src/ tests/

train:
 python scripts/experiments/run_baseline_comparison.py

evaluate:
 python scripts/experiments/run_robustness_test.py

mlflow-ui:
 mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0

docker-build:
 docker-compose -f docker/docker-compose.yml build

docker-up:
 docker-compose -f docker/docker-compose.yml up

clean:
 rm -rf __pycache__ .pytest_cache .mypy_cache htmlcov
 find . -type f -name "*.pyc" -delete
```

**Usage:**

```bash
make install    # Setup environment
make test       # Run tests
make train      # Train model
make mlflow-ui  # Start MLflow UI
```

---

## ðŸš¨ **CRITICAL COMPONENTS OFTEN FORGOTTEN**

### 1. **`.env.example` - Environment Variables**

```bash
# MLflow
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_S3_ENDPOINT_URL=https://s3.amazonaws.com

# AWS
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret

# Database
DB_HOST=localhost
DB_PORT=5432
```

### 2. **`.pre-commit-config.yaml` - Code Quality**

```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: ['--max-line-length=100']

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.0.0
    hooks:
      - id: mypy
```

### 3. **`pyproject.toml` - Modern Python Packaging**

```toml
[tool.poetry]
name = "rl-cdss-prescription-safety"
version = "1.0.0"
description = "RL-based CDSS for prescription safety under uncertainty"
authors = ["Herald Ginting <heraldmsamueltheo@gmail.com>"]

[tool.poetry.dependencies]
python = "^3.9"
numpy = "^1.21.0"
pandas = "^1.3.0"
mlflow = "^2.0.0"
dvc = "^3.0.0"

[tool.black]
line-length = 100
target-version = ['py39']

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
```

---

## ðŸŽ¯ **MLflow Experiment Management Workflow**

### **Scenario: Running Ablation Study**

**Goal:** Compare Q-Learning vs SARSA across 3 missing data rates

```bash
# 1. Define experiments in configs/
configs/experiment/
â”œâ”€â”€ q_learning_missing_20.yaml
â”œâ”€â”€ q_learning_missing_40.yaml
â”œâ”€â”€ q_learning_missing_60.yaml
â”œâ”€â”€ sarsa_missing_20.yaml
â”œâ”€â”€ sarsa_missing_40.yaml
â””â”€â”€ sarsa_missing_60.yaml

# 2. Run experiments (auto-logs to MLflow)
for config in configs/experiment/*.yaml; do
    python scripts/experiments/run_experiment.py --config $config
done

# 3. View in MLflow UI
mlflow ui

# 4. Compare runs visually
# - Select all 6 runs
# - Click "Compare"
# - View parallel coordinates plot

# 5. Promote best model to production
mlflow models serve -m "models:/q_learning_agent/Production" -p 5001
```

**MLflow UI Shows:**

- Detection rate trend across missing rates
- Learning curves side-by-side
- Hyperparameter impact (alpha, gamma)
- Artifact links (plots, models)

---

## âœ… **PRODUCTION READINESS CHECKLIST**

- [x] **Code Quality:** Linting (flake8), formatting (black), type hints (mypy)
- [x] **Testing:** Unit tests (>80% coverage), integration tests, CI/CD
- [x] **Experiment Tracking:** MLflow integration, config management
- [x] **Data Versioning:** DVC pipelines, remote storage
- [x] **Containerization:** Docker multi-stage builds, docker-compose
- [x] **Documentation:** API docs, architecture diagrams, user guides
- [x] **Reproducibility:** Configs, seeds, DVC lock files
- [x] **Monitoring:** Logging, metrics collection (future: Prometheus/Grafana)
- [x] **Deployment:** Model registry, API serving (FastAPI)
- [x] **Security:** `.env` for secrets, `.gitignore` for sensitive files

---

## ðŸš€ **NEXT STEPS: Implementing This Structure**

```bash
# 1. Create folders
mkdir -p configs/{experiment,data,deployment}
mkdir -p data/{raw,processed,synthetic,external}
mkdir -p src/{knowledge,environment,agents,training,evaluation,explainability,utils,api}
mkdir -p tests/{unit,integration,performance,fixtures}
mkdir -p docker scripts/{data_acquisition,preprocessing,experiments,deployment}
mkdir -p results/{figures,metrics,reports}

# 2. Initialize DVC & Git
git init
dvc init
git add .
git commit -m "feat: Initialize production MLOps structure"

# 3. Set up pre-commit hooks
pip install pre-commit
pre-commit install

# 4. Start MLflow server
make mlflow-ui
```

**Time to Production:** ~2 weeks with this structure (vs 2+ months ad-hoc)

---

**This is enterprise-grade MLOps structure** used by teams at Google, Meta, Spotify for scalable ML systems. ðŸŽ¯
